<?xml version="1.0" encoding="UTF-8"?>
<job>
  <name>pub_ods</name>
  <description />
  <extended_description />
  <job_version />
  <job_status>0</job_status>
  <directory>/</directory>
  <created_user>-</created_user>
  <created_date>2015/09/14 20:05:34.280</created_date>
  <modified_user>-</modified_user>
  <modified_date>2015/09/14 20:05:34.280</modified_date>
  <parameters>
    <parameter>
      <name>KETTLE_JOB_NAME</name>
      <default_value />
      <description />
    </parameter>
  </parameters>
  <connection>
    <name>hive</name>
    <server>${CMALDW_HIVE_DB_HOST}</server>
    <type>HIVE2</type>
    <access>Native</access>
    <database>${CMALDW_HIVE_DB_NAME}</database>
    <port>${CMALDW_HIVE_DB_PORT}</port>
    <username>${CMALDW_HIVE_DB_USER}</username>
    <password>${CMALDW_HIVE_DB_PASSWORD}</password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>INITIAL_POOL_SIZE</code>
        <attribute>1</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>MAXIMUM_POOL_SIZE</code>
        <attribute>1</attribute>
      </attribute>
      <attribute>
        <code>POOLING_initialSize</code>
        <attribute>1</attribute>
      </attribute>
      <attribute>
        <code>POOLING_maxActive</code>
        <attribute>1</attribute>
      </attribute>
      <attribute>
        <code>POOLING_maxIdle</code>
        <attribute>1</attribute>
      </attribute>
      <attribute>
        <code>POOLING_maxWait</code>
        <attribute>-1</attribute>
      </attribute>
      <attribute>
        <code>POOLING_testOnBorrow</code>
        <attribute>true</attribute>
      </attribute>
      <attribute>
        <code>POOLING_validationQuery</code>
        <attribute>select 1</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>${CMALDW_HIVE_DB_PORT}</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>Y</attribute>
      </attribute>
    </attributes>
  </connection>
  <slaveservers>
    </slaveservers>
  <job-log-table>
    <connection />
    <schema />
    <table />
    <size_limit_lines />
    <interval />
    <timeout_days />
    <field>
      <id>ID_JOB</id>
      <enabled>Y</enabled>
      <name>ID_JOB</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>JOBNAME</name>
    </field>
    <field>
      <id>STATUS</id>
      <enabled>Y</enabled>
      <name>STATUS</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>STARTDATE</id>
      <enabled>Y</enabled>
      <name>STARTDATE</name>
    </field>
    <field>
      <id>ENDDATE</id>
      <enabled>Y</enabled>
      <name>ENDDATE</name>
    </field>
    <field>
      <id>LOGDATE</id>
      <enabled>Y</enabled>
      <name>LOGDATE</name>
    </field>
    <field>
      <id>DEPDATE</id>
      <enabled>Y</enabled>
      <name>DEPDATE</name>
    </field>
    <field>
      <id>REPLAYDATE</id>
      <enabled>Y</enabled>
      <name>REPLAYDATE</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>Y</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>EXECUTING_SERVER</id>
      <enabled>N</enabled>
      <name>EXECUTING_SERVER</name>
    </field>
    <field>
      <id>EXECUTING_USER</id>
      <enabled>N</enabled>
      <name>EXECUTING_USER</name>
    </field>
    <field>
      <id>START_JOB_ENTRY</id>
      <enabled>N</enabled>
      <name>START_JOB_ENTRY</name>
    </field>
    <field>
      <id>CLIENT</id>
      <enabled>N</enabled>
      <name>CLIENT</name>
    </field>
  </job-log-table>
  <jobentry-log-table>
    <connection />
    <schema />
    <table />
    <timeout_days />
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>TRANSNAME</name>
    </field>
    <field>
      <id>JOBENTRYNAME</id>
      <enabled>Y</enabled>
      <name>STEPNAME</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>RESULT</id>
      <enabled>Y</enabled>
      <name>RESULT</name>
    </field>
    <field>
      <id>NR_RESULT_ROWS</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_ROWS</name>
    </field>
    <field>
      <id>NR_RESULT_FILES</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_FILES</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>N</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>COPY_NR</id>
      <enabled>N</enabled>
      <name>COPY_NR</name>
    </field>
  </jobentry-log-table>
  <channel-log-table>
    <connection />
    <schema />
    <table />
    <timeout_days />
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>LOGGING_OBJECT_TYPE</id>
      <enabled>Y</enabled>
      <name>LOGGING_OBJECT_TYPE</name>
    </field>
    <field>
      <id>OBJECT_NAME</id>
      <enabled>Y</enabled>
      <name>OBJECT_NAME</name>
    </field>
    <field>
      <id>OBJECT_COPY</id>
      <enabled>Y</enabled>
      <name>OBJECT_COPY</name>
    </field>
    <field>
      <id>REPOSITORY_DIRECTORY</id>
      <enabled>Y</enabled>
      <name>REPOSITORY_DIRECTORY</name>
    </field>
    <field>
      <id>FILENAME</id>
      <enabled>Y</enabled>
      <name>FILENAME</name>
    </field>
    <field>
      <id>OBJECT_ID</id>
      <enabled>Y</enabled>
      <name>OBJECT_ID</name>
    </field>
    <field>
      <id>OBJECT_REVISION</id>
      <enabled>Y</enabled>
      <name>OBJECT_REVISION</name>
    </field>
    <field>
      <id>PARENT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>PARENT_CHANNEL_ID</name>
    </field>
    <field>
      <id>ROOT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>ROOT_CHANNEL_ID</name>
    </field>
  </channel-log-table>
  <pass_batchid>N</pass_batchid>
  <shared_objects_file />
  <entries>
    <entry>
      <name>START</name>
      <description />
      <type>SPECIAL</type>
      <start>Y</start>
      <dummy>N</dummy>
      <repeat>N</repeat>
      <schedulerType>0</schedulerType>
      <intervalSeconds>0</intervalSeconds>
      <intervalMinutes>60</intervalMinutes>
      <hour>12</hour>
      <minutes>0</minutes>
      <weekDay>1</weekDay>
      <DayOfMonth>1</DayOfMonth>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>48</xloc>
      <yloc>48</yloc>
    </entry>
    <entry>
      <name>to_ods_inc_u_ora</name>
      <description />
      <type>SQL</type>
      <sql>set hive.execution.engine = ${SET_ENGINE};
set mapreduce.job.queuename=${CMALDW_ODS_QUEUE};
set spark.app.name = ${JOB_NAME};
set mapred.job.name = ${JOB_NAME};

INSERT OVERWRITE TABLE ${TAR_ODS_TAB_NAME}
SELECT a.* 
FROM ${TAR_HIS_TAB_NAME} a 
LEFT OUTER JOIN ${TAR_STG_TAB_NAME} b 
ON ${JOIN_CONDITION}
WHERE b.${SPLIT_BY} IS NULL
;

INSERT INTO TABLE ${TAR_ODS_TAB_NAME}
SELECT * FROM ${TAR_STG_TAB_NAME}
;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename />
      <sendOneStatement>F</sendOneStatement>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>640</xloc>
      <yloc>176</yloc>
    </entry>
    <entry>
      <name>to_ods_all_ora</name>
      <description />
      <type>SQL</type>
      <sql>set hive.execution.engine = ${SET_ENGINE};
set mapreduce.job.queuename=${CMALDW_ODS_QUEUE};
set spark.app.name = ${JOB_NAME};
set mapred.job.name = ${JOB_NAME};

INSERT OVERWRITE TABLE ${TAR_ODS_TAB_NAME}
SELECT * FROM ${TAR_STG_TAB_NAME}
;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename />
      <sendOneStatement>F</sendOneStatement>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>640</xloc>
      <yloc>48</yloc>
    </entry>
    <entry>
      <name>to_his_inc_ora</name>
      <description />
      <type>SQL</type>
      <sql>set hive.execution.engine = ${SET_ENGINE};
set mapreduce.job.queuename=${CMALDW_ODS_QUEUE};
set spark.app.name = ${JOB_NAME};
set mapred.job.name = ${JOB_NAME};

INSERT OVERWRITE TABLE ${TAR_HIS_TAB_NAME}
SELECT * FROM ${TAR_ODS_TAB_NAME}
;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename />
      <sendOneStatement>F</sendOneStatement>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>832</xloc>
      <yloc>176</yloc>
    </entry>
    <entry>
      <name>to_ods_inc_i_ora</name>
      <description />
      <type>SQL</type>
      <sql>set hive.execution.engine = ${SET_ENGINE};
set mapreduce.job.queuename=${CMALDW_ODS_QUEUE};
set spark.app.name = ${JOB_NAME};
set mapred.job.name = ${JOB_NAME};

INSERT OVERWRITE TABLE ${TAR_ODS_TAB_NAME} PARTITION (PART_DT='${START_DATE}')
SELECT * FROM ${TAR_STG_TAB_NAME}
;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename />
      <sendOneStatement>F</sendOneStatement>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>640</xloc>
      <yloc>304</yloc>
    </entry>
    <entry>
      <name>update_para</name>
      <description />
      <type>TRANS</type>
      <specification_method>filename</specification_method>
      <trans_object_id />
      <filename>${CMALDW_ETL_HOME}/1_ODS/ODS_ZCOTM/public_update_parameter_ods.ktr</filename>
      <transname />
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>N</set_logfile>
      <logfile />
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Basic</loglevel>
      <cluster>N</cluster>
      <slave_server_name />
      <set_append_logfile>N</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>N</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>1024</xloc>
      <yloc>208</yloc>
    </entry>
    <entry>
      <name>get_para</name>
      <description />
      <type>TRANS</type>
      <specification_method>filename</specification_method>
      <trans_object_id />
      <filename>${CMALDW_ETL_HOME}/1_ODS/ODS_ZCOTM/public_generate_parameter_ods.ktr</filename>
      <transname />
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>N</set_logfile>
      <logfile />
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Basic</loglevel>
      <cluster>N</cluster>
      <slave_server_name />
      <set_append_logfile>N</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>N</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>144</xloc>
      <yloc>48</yloc>
    </entry>
    <entry>
      <name>check_extract_type_ora</name>
      <description />
      <type>SIMPLE_EVAL</type>
      <valuetype>variable</valuetype>
      <fieldname />
      <variablename>${EXTRACT_MODE}</variablename>
      <fieldtype>string</fieldtype>
      <mask />
      <comparevalue>0</comparevalue>
      <minvalue />
      <maxvalue />
      <successcondition>startswith</successcondition>
      <successnumbercondition>equal</successnumbercondition>
      <successbooleancondition>false</successbooleancondition>
      <successwhenvarset>N</successwhenvarset>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>256</xloc>
      <yloc>48</yloc>
    </entry>
    <entry>
      <name>check_inc_insert_type_ora</name>
      <description />
      <type>SIMPLE_EVAL</type>
      <valuetype>variable</valuetype>
      <fieldname />
      <variablename>${EXTRACT_MODE}</variablename>
      <fieldtype>string</fieldtype>
      <mask />
      <comparevalue>1</comparevalue>
      <minvalue />
      <maxvalue />
      <successcondition>startswith</successcondition>
      <successnumbercondition>equal</successnumbercondition>
      <successbooleancondition>false</successbooleancondition>
      <successwhenvarset>N</successwhenvarset>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>256</xloc>
      <yloc>176</yloc>
    </entry>
    <entry>
      <name>sqoop_inc_u_ora</name>
      <description />
      <type>SHELL</type>
      <filename />
      <work_directory>${CMALDW_WORKING_DIR}</work_directory>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile />
      <set_append_logfile>N</set_append_logfile>
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>sqoop import  -D mapreduce.job.queuename=${CMALDW_ODS_QUEUE}  --hive-import --where "${INCREMENT_FIELD} >= to_date('${START_DATE}', 'yyyy-mm-dd') AND ${INCREMENT_FIELD} &lt; to_date('${END_DATE}', 'yyyy-mm-dd')"  --connect ${DB_JDBC}  --username ${DB_USER}  --password ${DB_PASSWORD} --table ${SRC_TABLE_NAME} --hive-table ${TAR_STG_TAB_NAME}  --delete-target-dir  --null-string '\\N'  --null-non-string '\\N'  --fields-terminated-by ${TERMINATED_BY}  --hive-delims-replacement ""  --hive-overwrite --split-by ${SPLIT_BY} -m ${PARAR_NUM}   --target-dir /user/hive/temp/${KETTLE_JOB_NAME}  --fetch-size ${FETCH_SIZE}  --mapreduce-job-name ${JOB_NAME}</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>432</xloc>
      <yloc>176</yloc>
    </entry>
    <entry>
      <name>sqoop_inc_i_ora</name>
      <description />
      <type>SHELL</type>
      <filename />
      <work_directory>${CMALDW_WORKING_DIR}</work_directory>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile />
      <set_append_logfile>N</set_append_logfile>
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>sqoop import -D mapreduce.job.queuename=${CMALDW_ODS_QUEUE}  --hive-import --where "${INCREMENT_FIELD} >= to_date('${START_DATE}', 'yyyy-mm-dd') AND ${INCREMENT_FIELD} &lt; to_date('${END_DATE}', 'yyyy-mm-dd')"  --connect ${DB_JDBC}  --username ${DB_USER}  --password ${DB_PASSWORD} --table ${SRC_TABLE_NAME} --hive-table ${TAR_STG_TAB_NAME}  --delete-target-dir  --null-string '\\N'  --null-non-string '\\N'  --fields-terminated-by ${TERMINATED_BY}  --hive-delims-replacement ""  --hive-overwrite --split-by ${SPLIT_BY} -m ${PARAR_NUM}   --target-dir /user/hive/temp/${KETTLE_JOB_NAME}   --fetch-size ${FETCH_SIZE}  --mapreduce-job-name ${JOB_NAME}</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>432</xloc>
      <yloc>304</yloc>
    </entry>
    <entry>
      <name>sqoop_all_ora</name>
      <description />
      <type>SHELL</type>
      <filename />
      <work_directory>${CMALDW_WORKING_DIR}</work_directory>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile />
      <set_append_logfile>N</set_append_logfile>
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>sqoop import  -D mapreduce.job.queuename=${CMALDW_ODS_QUEUE}  --hive-import --connect ${DB_JDBC}  --username ${DB_USER}  --password ${DB_PASSWORD} --table ${SRC_TABLE_NAME} --hive-table ${TAR_STG_TAB_NAME}  --delete-target-dir  --null-string '\\N'  --null-non-string '\\N'  --fields-terminated-by ${TERMINATED_BY}  --hive-delims-replacement ""  --hive-overwrite --split-by ${SPLIT_BY} -m ${PARAR_NUM}   --target-dir /user/hive/temp/${KETTLE_JOB_NAME}  --fetch-size ${FETCH_SIZE}  --mapreduce-job-name ${JOB_NAME}</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>432</xloc>
      <yloc>48</yloc>
    </entry>
    <entry>
      <name>SUCCESS</name>
      <description />
      <type>SUCCESS</type>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>1136</xloc>
      <yloc>208</yloc>
    </entry>
    <entry>
      <name>check_inc_insert_type_query</name>
      <description />
      <type>SIMPLE_EVAL</type>
      <valuetype>variable</valuetype>
      <fieldname />
      <variablename>${EXTRACT_MODE}</variablename>
      <fieldtype>string</fieldtype>
      <mask />
      <comparevalue>2</comparevalue>
      <minvalue />
      <maxvalue />
      <successcondition>startswith</successcondition>
      <successnumbercondition>equal</successnumbercondition>
      <successbooleancondition>false</successbooleancondition>
      <successwhenvarset>N</successwhenvarset>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>256</xloc>
      <yloc>304</yloc>
    </entry>
    <entry>
      <name>check_inc_query</name>
      <description />
      <type>SIMPLE_EVAL</type>
      <valuetype>variable</valuetype>
      <fieldname />
      <variablename>${EXTRACT_MODE}</variablename>
      <fieldtype>string</fieldtype>
      <mask />
      <comparevalue>3</comparevalue>
      <minvalue />
      <maxvalue />
      <successcondition>startswith</successcondition>
      <successnumbercondition>equal</successnumbercondition>
      <successbooleancondition>false</successbooleancondition>
      <successwhenvarset>N</successwhenvarset>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>256</xloc>
      <yloc>448</yloc>
    </entry>
    <entry>
      <name>sqoop_all_ora_query</name>
      <description />
      <type>SHELL</type>
      <filename />
      <work_directory>${CMALDW_WORKING_DIR}</work_directory>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile />
      <set_append_logfile>N</set_append_logfile>
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>sqoop import -D mapreduce.job.queuename=${CMALDW_ODS_QUEUE}  --hive-import --connect ${DB_JDBC}  --username ${DB_USER}  --password ${DB_PASSWORD} --query ${QUERY_STATEMENT} --hive-table ${TAR_STG_TAB_NAME}  --delete-target-dir  --null-string '\\N'  --null-non-string '\\N'  --fields-terminated-by ${TERMINATED_BY}  --hive-delims-replacement ""  --hive-overwrite --split-by ${SPLIT_BY} -m ${PARAR_NUM}   --target-dir /user/hive/temp/${KETTLE_JOB_NAME}   --fetch-size ${FETCH_SIZE}  --mapreduce-job-name ${JOB_NAME}</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>432</xloc>
      <yloc>384</yloc>
    </entry>
    <entry>
      <name>to_ods_all_ora_query</name>
      <description />
      <type>SQL</type>
      <sql>set hive.execution.engine = ${SET_ENGINE};
set mapreduce.job.queuename=${CMALDW_ODS_QUEUE};
set spark.app.name = ${JOB_NAME};
set mapred.job.name = ${JOB_NAME};

INSERT OVERWRITE TABLE ${TAR_ODS_TAB_NAME}
SELECT * FROM ${TAR_STG_TAB_NAME}
;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename />
      <sendOneStatement>F</sendOneStatement>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>640</xloc>
      <yloc>384</yloc>
    </entry>
    <entry>
      <name>to_ods_inc_u_ora_query</name>
      <description />
      <type>SQL</type>
      <sql>set hive.execution.engine = ${SET_ENGINE};
set mapreduce.job.queuename=${CMALDW_ODS_QUEUE};
set spark.app.name = ${JOB_NAME};
set mapred.job.name = ${JOB_NAME};

INSERT OVERWRITE TABLE ${TAR_ODS_TAB_NAME}
SELECT a.* 
FROM ${TAR_HIS_TAB_NAME} a 
LEFT OUTER JOIN ${TAR_STG_TAB_NAME} b 
ON ${JOIN_CONDITION}
WHERE b.${SPLIT_BY} IS NULL
;

INSERT INTO TABLE ${TAR_ODS_TAB_NAME}
SELECT * FROM ${TAR_STG_TAB_NAME}
;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename />
      <sendOneStatement>F</sendOneStatement>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>640</xloc>
      <yloc>464</yloc>
    </entry>
    <entry>
      <name>to_his_inc_ora_query</name>
      <description />
      <type>SQL</type>
      <sql>set hive.execution.engine = ${SET_ENGINE};
set mapreduce.job.queuename=${CMALDW_ODS_QUEUE};
set spark.app.name = ${JOB_NAME};
set mapred.job.name = ${JOB_NAME};

INSERT OVERWRITE TABLE ${TAR_HIS_TAB_NAME}
SELECT * FROM ${TAR_ODS_TAB_NAME}
;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename />
      <sendOneStatement>F</sendOneStatement>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>832</xloc>
      <yloc>464</yloc>
    </entry>
    <entry>
      <name>sqoop_inc_u_ora_query</name>
      <description />
      <type>SHELL</type>
      <filename />
      <work_directory>${CMALDW_WORKING_DIR}</work_directory>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile />
      <set_append_logfile>N</set_append_logfile>
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>if [ ${SRC_DB_TYPE}x = "SQLSERVER"x ]; then
#sqoop import  --hive-import --where "${INCREMENT_FIELD} >= cast('${START_DATE}' as datetime) AND ${INCREMENT_FIELD} &lt; cast('${END_DATE}' as datetime)"  --connect ${DB_JDBC}  --username ${DB_USER}  --password ${DB_PASSWORD} --query '${QUERY_STATEMENT}' --hive-table ${TAR_STG_TAB_NAME}  --delete-target-dir  --null-string '\\N'  --null-non-string '\\N'  --fields-terminated-by ${TERMINATED_BY}  --hive-delims-replacement ""  --hive-overwrite --split-by ${SPLIT_BY} -m ${PARAR_NUM}   --target-dir /user/hive/temp/${KETTLE_JOB_NAME}  --fetch-size ${FETCH_SIZE}  --mapreduce-job-name ${JOB_NAME}
sqoop import -D mapreduce.job.queuename=${CMALDW_ODS_QUEUE} --hive-import --connect ${DB_JDBC}  --username ${DB_USER}  --password ${DB_PASSWORD} --query "${QUERY_STATEMENT} AND ${INCREMENT_FIELD} >= cast('${START_DATE}' as datetime) AND ${INCREMENT_FIELD} &lt; cast('${END_DATE}' as datetime)" --hive-table ${TAR_STG_TAB_NAME}  --delete-target-dir  --null-string '\\N'  --null-non-string '\\N'  --fields-terminated-by ${TERMINATED_BY}  --hive-delims-replacement ""  --hive-overwrite --split-by ${SPLIT_BY} -m ${PARAR_NUM}   --target-dir /user/hive/temp/${KETTLE_JOB_NAME}  --fetch-size ${FETCH_SIZE}  --mapreduce-job-name ${JOB_NAME}
elif [ ${SRC_DB_TYPE}x = "MYSQL"x ]; then
# Mysql
#sqoop import  --hive-import --where "${INCREMENT_FIELD} >= str_to_date('${START_DATE}', '%Y-%m-%d') AND ${INCREMENT_FIELD} &lt; str_to_date('${END_DATE}', '%Y-%m-%d')"  --connect ${DB_JDBC}  --username ${DB_USER}  --password ${DB_PASSWORD} --query '${QUERY_STATEMENT}' --hive-table ${TAR_STG_TAB_NAME}  --delete-target-dir  --null-string '\\N'  --null-non-string '\\N'  --fields-terminated-by ${TERMINATED_BY}  --hive-delims-replacement ""  --hive-overwrite --split-by ${SPLIT_BY} -m ${PARAR_NUM}   --target-dir /user/hive/temp/${KETTLE_JOB_NAME}  --fetch-size ${FETCH_SIZE}  --mapreduce-job-name ${JOB_NAME}
sqoop import  -D mapreduce.job.queuename=${CMALDW_ODS_QUEUE} --hive-import --connect ${DB_JDBC}  --username ${DB_USER}  --password ${DB_PASSWORD} --query "${QUERY_STATEMENT} AND ${INCREMENT_FIELD} >= str_to_date('${START_DATE}', '%Y-%m-%d') AND ${INCREMENT_FIELD} &lt; str_to_date('${END_DATE}', '%Y-%m-%d')" --hive-table ${TAR_STG_TAB_NAME}  --delete-target-dir  --null-string '\\N'  --null-non-string '\\N'  --fields-terminated-by ${TERMINATED_BY}  --hive-delims-replacement ""  --hive-overwrite --split-by ${SPLIT_BY} -m ${PARAR_NUM}   --target-dir /user/hive/temp/${KETTLE_JOB_NAME}  --fetch-size ${FETCH_SIZE}  --mapreduce-job-name ${JOB_NAME}
else
# Oracle 
sqoop import  -D mapreduce.job.queuename=${CMALDW_ODS_QUEUE} --hive-import --connect ${DB_JDBC}  --username ${DB_USER}  --password ${DB_PASSWORD} --query "${QUERY_STATEMENT} AND ${INCREMENT_FIELD} >= to_date('${START_DATE}', 'yyyy-mm-dd') AND ${INCREMENT_FIELD} &lt; to_date('${END_DATE}', 'yyyy-mm-dd')" --hive-table ${TAR_STG_TAB_NAME}  --delete-target-dir  --null-string '\\N'  --null-non-string '\\N'  --fields-terminated-by ${TERMINATED_BY}  --hive-delims-replacement ""  --hive-overwrite --split-by ${SPLIT_BY} -m ${PARAR_NUM}   --target-dir /user/hive/temp/${KETTLE_JOB_NAME}  --fetch-size ${FETCH_SIZE}  --mapreduce-job-name ${JOB_NAME}
fi</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>448</xloc>
      <yloc>496</yloc>
    </entry>
    <entry>
      <name>check_extract_mode_4</name>
      <description />
      <type>SIMPLE_EVAL</type>
      <valuetype>variable</valuetype>
      <fieldname />
      <variablename>${EXTRACT_MODE}</variablename>
      <fieldtype>string</fieldtype>
      <mask />
      <comparevalue>4</comparevalue>
      <minvalue />
      <maxvalue />
      <successcondition>startswith</successcondition>
      <successnumbercondition>equal</successnumbercondition>
      <successbooleancondition>false</successbooleancondition>
      <successwhenvarset>N</successwhenvarset>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>256</xloc>
      <yloc>592</yloc>
    </entry>
    <entry>
      <name>sqoop_all_ora_dmp</name>
      <description />
      <type>SHELL</type>
      <filename />
      <work_directory>${CMALDW_WORKING_DIR}</work_directory>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile />
      <set_append_logfile>N</set_append_logfile>
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>sqoop import -D mapreduce.job.queuename=${CMALDW_ODS_QUEUE}  --hive-import --connect ${DB_JDBC}  --username ${DB_USER}  --password ${DB_PASSWORD} --table ${SRC_TABLE_NAME} --hive-table ${TAR_STG_TAB_NAME}  --delete-target-dir  --null-string '\\N'  --null-non-string '\\N'  --fields-terminated-by ${TERMINATED_BY}  --hive-delims-replacement ""  --hive-overwrite --split-by ${SPLIT_BY} -m ${PARAR_NUM}   --target-dir ${TARGET_DIR}   --fetch-size ${FETCH_SIZE}  --mapreduce-job-name ${JOB_NAME}</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>976</xloc>
      <yloc>768</yloc>
    </entry>
    <entry>
      <name>check_table_if_exist</name>
      <description />
      <type>TABLE_EXISTS</type>
      <tablename>${CMALDW_TAR_ODS_HIS_TABLE}</tablename>
      <schemaname>${CMALDW_TAR_ODS_HIS_SCHEMA}</schemaname>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>832</xloc>
      <yloc>48</yloc>
    </entry>
    <entry>
      <name>to_his_all_init</name>
      <description />
      <type>SQL</type>
      <sql>set hive.execution.engine = ${SET_ENGINE};
set mapreduce.job.queuename=${CMALDW_ODS_QUEUE};
set spark.app.name = ${JOB_NAME};
set mapred.job.name = ${JOB_NAME};

INSERT OVERWRITE TABLE ${TAR_HIS_TAB_NAME}
SELECT * FROM ${TAR_ODS_TAB_NAME}
;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename />
      <sendOneStatement>F</sendOneStatement>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>1024</xloc>
      <yloc>48</yloc>
    </entry>
    <entry>
      <name>check_table_if_exist_3</name>
      <description />
      <type>TABLE_EXISTS</type>
      <tablename>${CMALDW_TAR_ODS_HIS_TABLE}</tablename>
      <schemaname>${CMALDW_TAR_ODS_HIS_SCHEMA}</schemaname>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>1</nr>
      <xloc>832</xloc>
      <yloc>384</yloc>
    </entry>
    <entry>
      <name>to_his_all_init_3</name>
      <description />
      <type>SQL</type>
      <sql>set hive.execution.engine = ${SET_ENGINE};
set mapreduce.job.queuename=${CMALDW_ODS_QUEUE};
set spark.app.name = ${JOB_NAME};
set mapred.job.name = ${JOB_NAME};

INSERT OVERWRITE TABLE ${TAR_HIS_TAB_NAME}
SELECT * FROM ${TAR_ODS_TAB_NAME}
;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename />
      <sendOneStatement>F</sendOneStatement>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>1</nr>
      <xloc>832</xloc>
      <yloc>304</yloc>
    </entry>
    <entry>
      <name>check_extract_mode_6</name>
      <description />
      <type>SIMPLE_EVAL</type>
      <valuetype>variable</valuetype>
      <fieldname />
      <variablename>${EXTRACT_MODE}</variablename>
      <fieldtype>string</fieldtype>
      <mask />
      <comparevalue>6</comparevalue>
      <minvalue />
      <maxvalue />
      <successcondition>startswith</successcondition>
      <successnumbercondition>equal</successnumbercondition>
      <successbooleancondition>false</successbooleancondition>
      <successwhenvarset>N</successwhenvarset>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>416</xloc>
      <yloc>640</yloc>
    </entry>
    <entry>
      <name>sqoop_inc_u_ora_query 2</name>
      <description />
      <type>SHELL</type>
      <filename />
      <work_directory>${CMALDW_WORKING_DIR}</work_directory>
      <arg_from_previous>N</arg_from_previous>
      <exec_per_row>N</exec_per_row>
      <set_logfile>N</set_logfile>
      <logfile />
      <set_append_logfile>N</set_append_logfile>
      <logext />
      <add_date>N</add_date>
      <add_time>N</add_time>
      <insertScript>Y</insertScript>
      <script>sqoop import -D mapreduce.job.queuename=${CMALDW_ODS_QUEUE}  --hive-import --connect ${DB_JDBC}  --username ${DB_USER}  --password ${DB_PASSWORD} --query ${QUERY_STATEMENT} --hive-table ${TAR_STG_TAB_NAME}  --delete-target-dir  --null-string '\\N'  --null-non-string '\\N'  --fields-terminated-by ${TERMINATED_BY}  --hive-delims-replacement ""  --hive-overwrite --split-by ${SPLIT_BY} -m ${PARAR_NUM}   --target-dir /user/hive/temp/${KETTLE_JOB_NAME}   --fetch-size ${FETCH_SIZE}  --mapreduce-job-name ${JOB_NAME}</script>
      <loglevel>Basic</loglevel>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>608</xloc>
      <yloc>608</yloc>
    </entry>
    <entry>
      <name>to_ods_inc_u_ora_query 2</name>
      <description />
      <type>SQL</type>
      <sql>set hive.execution.engine = ${SET_ENGINE};
set mapreduce.job.queuename=${CMALDW_ODS_QUEUE};
set spark.app.name = ${JOB_NAME};
set mapred.job.name = ${JOB_NAME};

INSERT OVERWRITE TABLE ${TAR_ODS_TAB_NAME}
SELECT a.* 
FROM ${TAR_HIS_TAB_NAME} a 
LEFT OUTER JOIN ${TAR_STG_TAB_NAME} b 
ON ${JOIN_CONDITION}
WHERE b.${SPLIT_BY} IS NULL
;

INSERT INTO TABLE ${TAR_ODS_TAB_NAME}
SELECT * FROM ${TAR_STG_TAB_NAME}
;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename />
      <sendOneStatement>F</sendOneStatement>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>784</xloc>
      <yloc>576</yloc>
    </entry>
    <entry>
      <name>to_his_inc_ora_query 2</name>
      <description />
      <type>SQL</type>
      <sql>set hive.execution.engine = ${SET_ENGINE};
set mapreduce.job.queuename=${CMALDW_ODS_QUEUE};
set spark.app.name = ${JOB_NAME};
set mapred.job.name = ${JOB_NAME};

INSERT OVERWRITE TABLE ${TAR_HIS_TAB_NAME}
SELECT * FROM ${TAR_ODS_TAB_NAME}
;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename />
      <sendOneStatement>F</sendOneStatement>
      <connection>hive</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>944</xloc>
      <yloc>496</yloc>
    </entry>
  </entries>
  <hops>
    <hop>
      <from>to_ods_inc_u_ora</from>
      <to>to_his_inc_ora</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>to_his_inc_ora</from>
      <to>update_para</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>to_ods_inc_i_ora</from>
      <to>update_para</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>START</from>
      <to>get_para</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>check_extract_type_ora</from>
      <to>check_inc_insert_type_ora</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>sqoop_inc_u_ora</from>
      <to>to_ods_inc_u_ora</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_inc_insert_type_ora</from>
      <to>sqoop_inc_u_ora</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_extract_type_ora</from>
      <to>sqoop_all_ora</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>sqoop_inc_i_ora</from>
      <to>to_ods_inc_i_ora</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>sqoop_all_ora</from>
      <to>to_ods_all_ora</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>update_para</from>
      <to>SUCCESS</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>get_para</from>
      <to>check_extract_type_ora</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_inc_insert_type_ora</from>
      <to>check_inc_insert_type_query</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_inc_insert_type_query</from>
      <to>sqoop_inc_i_ora</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_inc_insert_type_query</from>
      <to>check_inc_query</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_inc_query</from>
      <to>sqoop_all_ora_query</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>sqoop_all_ora_query</from>
      <to>to_ods_all_ora_query</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>sqoop_inc_u_ora_query</from>
      <to>to_ods_inc_u_ora_query</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>to_ods_inc_u_ora_query</from>
      <to>to_his_inc_ora_query</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>to_his_inc_ora_query</from>
      <to>update_para</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_inc_query</from>
      <to>check_extract_mode_4</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>sqoop_all_ora_dmp</from>
      <to>update_para</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>to_ods_all_ora</from>
      <to>check_table_if_exist</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_extract_mode_4</from>
      <to>sqoop_inc_u_ora_query</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_table_if_exist</from>
      <to>update_para</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_table_if_exist</from>
      <to>to_his_all_init</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>to_his_all_init</from>
      <to>update_para</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>to_ods_all_ora_query</from>
      <to>check_table_if_exist_3</to>
      <from_nr>0</from_nr>
      <to_nr>1</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_table_if_exist_3</from>
      <to>to_his_all_init_3</to>
      <from_nr>1</from_nr>
      <to_nr>1</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>to_his_all_init_3</from>
      <to>update_para</to>
      <from_nr>1</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_table_if_exist_3</from>
      <to>update_para</to>
      <from_nr>1</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_extract_mode_4</from>
      <to>check_extract_mode_6</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_extract_mode_6</from>
      <to>sqoop_inc_u_ora_query 2</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>sqoop_inc_u_ora_query 2</from>
      <to>to_ods_inc_u_ora_query 2</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>to_ods_inc_u_ora_query 2</from>
      <to>to_his_inc_ora_query 2</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>to_his_inc_ora_query 2</from>
      <to>update_para</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>check_extract_mode_6</from>
      <to>sqoop_all_ora_dmp</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>N</evaluation>
      <unconditional>N</unconditional>
    </hop>
  </hops>
  <notepads>
    <notepad>
      <note>功能说明： ODS 公用转换
创建日期： 2017-09-19
备　　注：
EXTRACT_MODE值为  
0     全量              table方式
1     增量              table方式
2     增量（分区存储）   table方式
3     全量              query方式
4     增量              query方式
5     全量              无列格式指定方式
使用query方式，在sql末尾要加where $CONDITIONS
示例：
select * from ANY3YKT.ALLSCHEDULING
where $CONDITIONS</note>
      <xloc>0</xloc>
      <yloc>720</yloc>
      <width>360</width>
      <heigth>220</heigth>
      <fontname>.Helvetica Neue DeskInterface</fontname>
      <fontsize>11</fontsize>
      <fontbold>N</fontbold>
      <fontitalic>N</fontitalic>
      <fontcolorred>0</fontcolorred>
      <fontcolorgreen>0</fontcolorgreen>
      <fontcolorblue>0</fontcolorblue>
      <backgroundcolorred>255</backgroundcolorred>
      <backgroundcolorgreen>205</backgroundcolorgreen>
      <backgroundcolorblue>112</backgroundcolorblue>
      <bordercolorred>100</bordercolorred>
      <bordercolorgreen>100</bordercolorgreen>
      <bordercolorblue>100</bordercolorblue>
      <drawshadow>Y</drawshadow>
    </notepad>
  </notepads>
  <attributes>
    <group>
      <name>METASTORE.pentaho</name>
      <attribute>
        <key>Default Run Configuration</key>
        <value>{"namespace":"pentaho","id":"Default Run Configuration","name":"Default Run Configuration","description":"Defines a default run configuration","metaStoreName":null}</value>
      </attribute>
    </group>
    <group>
      <name>{"_":"Embedded MetaStore Elements","namespace":"pentaho","type":"Default Run Configuration"}</name>
      <attribute>
        <key>Pentaho local</key>
        <value>{"children":[{"children":[],"id":"server","value":null},{"children":[],"id":"clustered","value":"N"},{"children":[],"id":"name","value":"Pentaho local"},{"children":[],"id":"description","value":null},{"children":[],"id":"readOnly","value":"Y"},{"children":[],"id":"sendResources","value":"N"},{"children":[],"id":"logRemoteExecutionLocally","value":"N"},{"children":[],"id":"remote","value":"N"},{"children":[],"id":"local","value":"Y"},{"children":[],"id":"showTransformations","value":"N"}],"id":"Pentaho local","value":null,"name":"Pentaho local","owner":null,"ownerPermissionsList":[]}</value>
      </attribute>
    </group>
  </attributes>
</job>
